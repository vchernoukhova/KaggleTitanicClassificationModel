{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be able to run this script as module\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "a=2\n",
    "\n",
    "module_path = os.path.abspath(os.getcwd())    \n",
    "\n",
    "if module_path not in sys.path:       \n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowTypes(X):\n",
    "    \n",
    "    #train.dtypes\n",
    "    columns = list(X)\n",
    "\n",
    "    print ('\\033[1m', \"Categorical variables of the dataset are: \")\n",
    "    print ('\\033[0m')\n",
    "\n",
    "    categorical_variables = X.select_dtypes(include = 'object').columns.tolist()\n",
    "    print (* categorical_variables, sep = \"\\n\")\n",
    "            \n",
    "\n",
    "    print (' ')\n",
    "    print ('\\033[1m', \"Numerical variables of the dataset are: \")\n",
    "    print ('\\033[0m')\n",
    "\n",
    "    numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_variables = X.select_dtypes(include = numeric).columns.tolist()\n",
    "    \n",
    "    print(* numeric_variables, sep = \"\\n\")\n",
    "    \n",
    "    return\n",
    "\n",
    "def ShowLevels(X):\n",
    "    \n",
    "    categorical_variables = X.select_dtypes(include = 'object').columns.tolist()\n",
    "    \n",
    "    print(\"Number of records in the dataset is \",  X.shape[0]) \n",
    "    print ('='*50)\n",
    "    print(' ')\n",
    "    for variable in categorical_variables:\n",
    "        print (\"Number of \", variable, \" levels: \", X[variable].nunique()) \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(name):\n",
    "    \n",
    "    import re as re\n",
    "\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    \n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_name(name):\n",
    "    \n",
    "    last_name = name.split(',')[0]\n",
    "    \n",
    "    if last_name:\n",
    "        return last_name\n",
    "    \n",
    "    return\n",
    "\n",
    "def get_other_full_name(name):\n",
    "    \n",
    "    other_full_name = name[name.find(\"(\")+1 : name.find(\")\")]\n",
    "    \n",
    "    if name.find(\"(\") != -1:\n",
    "        return other_full_name\n",
    "    return\n",
    "\n",
    "def get_other_last_name(name):\n",
    "    \n",
    "    other_full_name = name[name.find(\"(\")+1 : name.find(\")\")]\n",
    "    \n",
    "    other_last_name = other_full_name.split(' ')[-1]\n",
    "    \n",
    "    if name.find(\"(\") != -1:\n",
    "        return other_last_name\n",
    "    return    \n",
    "\n",
    "def get_capital_letters(word):\n",
    "    \n",
    "    import re as re\n",
    "    capital_letters = re.search('[A-Z]+', word)\n",
    "    \n",
    "    if capital_letters:\n",
    "        return capital_letters.group(0)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_most_frequent (X, y):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    X_out = X\n",
    "    most_frequent = pd.get_dummies(X_out[y]).sum().sort_values(ascending=False).index[0]\n",
    "    X_out[y] = X_out[y].fillna(most_frequent)\n",
    "    \n",
    "    return X_out\n",
    "\n",
    "def impute_mean (X, y):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    \n",
    "    X_out = X\n",
    "    mean_imputer = Imputer (missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "    imputed_y = mean_imputer.fit_transform (X_out[[y]])\n",
    "    X_out[y] = imputed_y\n",
    "    \n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ffff(learning_rate_start,learning_rate_end,\n",
    "#             *n_estimators_start, n_estimators_end,\n",
    "#             max_depth_start, max_depth_end,\n",
    "#             min_samples_split_start, min_samples_split_end,\n",
    "#             min_samples_leaf_start, min_samples_leaf_end):\n",
    "    \n",
    "#     import numpy as np\n",
    "#     learning_rates = [float(x) for x in np.linspace(\n",
    "#         start = learning_rate_start,\n",
    "#         stop = learning_rate_end, \n",
    "#         num = 50)]\n",
    "\n",
    "#     n_estimators = [int(x) for x in np.linspace(\n",
    "#         start = n_estimators_start,\n",
    "#         stop = n_estimators_end,\n",
    "#         num = 50)]\n",
    "\n",
    "#     max_depth = [int(x) for x in np.linspace(\n",
    "#         start = max_depth_start, \n",
    "#         stop = max_depth_end, \n",
    "#         num = 50)]\n",
    "\n",
    "#     max_depth.append(None)\n",
    "\n",
    "#     min_samples_split = [int(x) for x in np.linspace(\n",
    "#         start = min_samples_split_start,\n",
    "#         stop = min_samples_split_end, \n",
    "#         num = 10)]\n",
    "\n",
    "#     min_samples_leaf = [int(x) for x in np.linspace(\n",
    "#         start = min_samples_leaf_start, \n",
    "#         stop = min_samples_leaf_end, \n",
    "#         num = 50)]\n",
    "    \n",
    "#     return learning_rates\n",
    "\n",
    "# Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'learning_rate': learning_rates}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_regressor(data, variable):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    #data.columns.tolist()\n",
    "    all_columns = ['Pclass', 'Sex', 'Age', 'SibSp','Parch',\n",
    "                         'FamilySize','Fare','Embarked','IsAlone','Title' ]\n",
    "    \n",
    "    data_for_var = data[all_columns]\n",
    "    \n",
    "    columns_to_check = all_columns\n",
    "    columns_to_check.remove(variable) # REMOVES IT FROM ALL COLUMNS TOO!!!!\n",
    "    \n",
    "    for column in columns_to_check:\n",
    "        if data_for_var[column].isna().sum() != 0:\n",
    "            data_for_var.drop (column, 1, inplace = True)\n",
    "\n",
    "    categorical_variables = data_for_var.select_dtypes(include = 'object').columns.tolist()\n",
    "    dummy_variables = pd.get_dummies(data_for_var.loc[:,categorical_variables])\n",
    "\n",
    "    data_for_var = pd.merge(data_for_var, dummy_variables, left_index = True, right_index = True)\n",
    "    data_for_var = data_for_var.drop(categorical_variables, axis = 1)\n",
    "\n",
    "\n",
    "    X_train = data_for_var.loc[data_for_var[variable].isna()==False, :]\n",
    "    X_train = X_train.drop (variable, 1)\n",
    "\n",
    "    y_train = data_for_var.loc[data_for_var[variable].isna()==False,variable]\n",
    "\n",
    "    X_test =  data_for_var.loc[data_for_var[variable].isna()==True,:]\n",
    "    X_test = X_test.drop (variable, 1)\n",
    "\n",
    "\n",
    "    classifier = RandomForestRegressor()\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    X_test[variable] = y_pred\n",
    "\n",
    "    data_out = data\n",
    "    data_out[variable].fillna(X_test[variable], inplace = True)\n",
    "    \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Create Binning ################################################\n",
    "\n",
    "# columns = list(data_train)\n",
    "# for variable in columns:\n",
    "#     if data_train[variable].nunique() < 10 and variable != 'Survived':\n",
    "#         print(data_train[[variable, 'Survived']].groupby([variable], as_index=False).mean().sort_values(by = variable, ascending = True))\n",
    "#         print(' ')\n",
    "\n",
    "#Harder decision for Titles\n",
    "#data_train[['Title', 'Survived']].groupby(['Title'], as_index=False).count().sort_values(by = 'Survived', ascending = False)\n",
    "\n",
    "\n",
    "# data_train['Fare_bands'] = pd.cut(data_train['Fare'], 10)\n",
    "# data_train[['Fare_bands', 'Survived']].groupby(['Fare_bands'], as_index=False).mean().sort_values(by = 'Fare_bands', ascending = True)\n",
    "\n",
    "# data_train['Age_bands'] = pd.cut(data_train['Age'], 10)\n",
    "# data_train[['Age_bands', 'Survived']].groupby(['Age_bands'], as_index=False).mean().sort_values(by = 'Age_bands', ascending = True)\n",
    "\n",
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binning(data):\n",
    "    \n",
    "    #SibSp\n",
    "    data.loc[data['SibSp'] == 0, 'SibSp_cat'] = '0'\n",
    "    data.loc[(data['SibSp'] >= 1) & (data['SibSp'] <= 2) , 'SibSp_cat'] = '1-2'\n",
    "    data.loc[(data['SibSp'] >= 3) & (data['SibSp'] <= 4) , 'SibSp_cat'] = '3-4'\n",
    "    data.loc[data['SibSp'] > 4 , 'SibSp_cat'] = '5+'\n",
    "\n",
    "    #Parch\n",
    "    data.loc[data['Parch'] == 0, 'Parch_cat'] = '0'\n",
    "    data.loc[(data['Parch'] >= 1) & (data['Parch'] <= 2) , 'Parch_cat'] = '1-2'\n",
    "    data.loc[data['Parch'] == 3 , 'Parch_cat'] = '3'\n",
    "    data.loc[data['Parch'] > 3 , 'Parch_cat'] = '4+'\n",
    "\n",
    "    #FamilySize\n",
    "    data.loc[data['FamilySize'] == 0, 'FamilySize_cat'] = '0'\n",
    "    data.loc[data['FamilySize'] == 1, 'FamilySize_cat'] = '1'\n",
    "    data.loc[(data['FamilySize'] >= 2) & (data['FamilySize'] <= 3) , 'FamilySize_cat'] = '2-3'\n",
    "    data.loc[data['FamilySize'] == 4, 'FamilySize_cat'] = '4'\n",
    "    data.loc[data['FamilySize'] > 4, 'FamilySize_cat'] = '5+'\n",
    "\n",
    "    #Age\n",
    "    data['Age_bands'] = pd.cut(data['Age'], 4)\n",
    "\n",
    "    data.loc[(data['Age'] >= 0) & (data['Age'] <= 16) , 'Age_cat'] = '0-16'\n",
    "    data.loc[(data['Age'] > 16) & (data['Age'] <= 50) , 'Age_cat'] = '16-50'\n",
    "    data.loc[(data['Age'] > 50) & (data['Age'] <= 60) , 'Age_cat'] = '50-60'\n",
    "    data.loc[data['Age'] > 60 , 'Age_cat'] = '60+'\n",
    "\n",
    "    #Fare\n",
    "    data['Fare_bands'] = pd.cut(data['Fare'], 10)\n",
    "\n",
    "    data.loc[data['Fare'] <= 51 , 'Fare_cat'] = '0-51'\n",
    "    data.loc[(data['Fare'] > 51) & (data['Fare'] <= 205) , 'Fare_cat'] = '51-205'\n",
    "    data.loc[(data['Fare'] > 205) & (data['Fare'] <= 307) , 'Fare_cat'] = '205-307'\n",
    "    data.loc[data['Fare'] > 307 , 'Fare_cat'] = '307+'\n",
    "    \n",
    "    data_out = data.drop(['SibSp', 'Parch', 'FamilySize', 'Age', 'Age_bands', 'Fare', 'Fare_bands'], axis = 1)\n",
    "    \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Deleting variables #############################################################\n",
    "\n",
    "#data_train = data_train.drop ('SibSp', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## Data Exporation #####################################################\n",
    "\n",
    "####### Group by each categorical column, and check the response\n",
    "\n",
    "# columns = list(data_before_dummies)\n",
    "# for variable in columns:\n",
    "#     if data_before_dummies[variable].nunique() < 10 and variable != 'Survived':\n",
    "#         print(data_before_dummies[[variable, 'Survived']].groupby([variable], as_index=False).mean())\n",
    "#         print(' ')\n",
    "\n",
    "####### Look at your variables, think what might be correlated or requires special attention\n",
    "#print ( *data_before_dummies.columns.tolist(), sep = \"\\n\")\n",
    "\n",
    "####### Heat map for variable correlations\n",
    "#sns.heatmap(data_before_dummies.corr(), vmax=1., square=False, cmap=\"YlGnBu\").xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data \n",
    "#data_before_processing = data\n",
    "#data_before_processing.to_csv('data_before_processing.csv', index = False, header = True)\n",
    "#data_before_processing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_var(data):\n",
    "    \n",
    "    import nbimporter\n",
    "    from setup_code import get_title\n",
    "    \n",
    "    #family size\n",
    "    data.loc[:,'FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
    "\n",
    "    #is alone\n",
    "    data.loc[:,'IsAlone'] = 0\n",
    "    data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1    \n",
    "\n",
    "    #title\n",
    "    data.loc[:,'Title'] = data['Name'].apply(get_title)\n",
    "\n",
    "    # Title\n",
    "    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Major', \n",
    "                                                  'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n",
    "    data['Title'] = data['Title'].replace('Ms', 'Miss')\n",
    "    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "    ###\n",
    "    data.loc[:,'Last_name'] = data['Name'].apply(get_last_name)\n",
    "    data.loc[:,'Other_full_name'] = data['Name'].apply(get_other_full_name)\n",
    "    data.loc[:,'Other_last_name'] = data['Name'].apply(get_other_last_name)\n",
    "    data.loc[data['Cabin'].isnull() == False,'Cabin_letter'] = data.loc[data['Cabin'].isnull() == False,'Cabin'].apply(get_capital_letters)\n",
    "    \n",
    "    \n",
    "    #Split for tickets\n",
    "    \n",
    "    #if there is a space, then split into two columns\n",
    "    data.loc[data['Ticket'].str.find(' ') != -1, 'Ticket_first'] = data['Ticket'].str.split().str[0]\n",
    "    data.loc[data['Ticket'].str.find(' ') != -1, 'Ticket_second'] = data['Ticket'].str.split().str[1]\n",
    "\n",
    "    #if no space, assign it to the second\n",
    "    data.loc[data['Ticket'].str.find(' ') == -1, 'Ticket_second'] = data['Ticket']\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_var_types(data):\n",
    "    \n",
    "    data_out = data\n",
    "    data_out = data_out.copy() # explanation - https://www.dataquest.io/blog/settingwithcopywarning/\n",
    "    \n",
    "    data_out.loc[:,'Pclass'] = data_out['Pclass'].astype('object')\n",
    "    data_out.loc[:,'IsAlone'] = data_out['IsAlone'].astype('object')\n",
    "    \n",
    "    data_out.loc[:,'SibSp'] = data_out['SibSp'].astype('object')\n",
    "    data_out.loc[data_out['SibSp'].isin ([0,1,2,3,4]) == False , 'SibSp'] = '5+'\n",
    "\n",
    "    data_out.loc[:,'Parch'] = data_out['Parch'].astype('object')\n",
    "    data_out.loc[data_out['Parch'].isin ([0,1,2,3]) == False , 'Parch']  = '4+'\n",
    "    \n",
    "    data_out.loc[:,'FamilySize'] = data_out['FamilySize'].astype('object')\n",
    "    data_out.loc[data_out['FamilySize'].isin ([0,1,2,3, 4]) == False , 'FamilySize']  = '5+'\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    data_out.loc[:,'Cabin_impact'] = data_out['Cabin_impact'].astype('float32')\n",
    "    data_out.loc[:,'Ticket_impact'] = data_out['Ticket_impact'].astype('float32')\n",
    "    data_out.loc[:,'Last_name_impact'] = data_out['Last_name_impact'].astype('float32')\n",
    "    data_out.loc[:,'Other_last_name_impact'] = data_out['Other_last_name_impact'].astype('float32')  \n",
    "    data_out.loc[:,'Ticket_second_impact'] = data_out['Ticket_second_impact'].astype('float32')\n",
    "    data_out.loc[:,'Ticket_first_impact'] = data_out['Ticket_first_impact'].astype('float32')\n",
    "\n",
    "    \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################  Imputing missing values ###############################################\n",
    "\n",
    "#data_train.isna().sum()\n",
    "\n",
    "# Survived, SibSp, Parch, IsAlone might have 0 and it makes sense\n",
    "\n",
    "#check_for_missing = data_train.loc[:, data_train.columns.isin(['Survived', 'SibSp', 'Parch', 'IsAlone'])==False]\n",
    "#check_for_missing.shape[0] - check_for_missing.fillna(0).astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_ind(data, variable, name):\n",
    "    \n",
    "    # Create a column for which ones values are missing\n",
    "    data.loc[data[variable].isna()==True,name] = 1\n",
    "    data.loc[data[variable].isna()==False,name] = 0\n",
    "\n",
    "    # Make it categorical\n",
    "    data.loc[:,name] = data[name].astype('object')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(data, to_print):\n",
    "    \n",
    "    categorical_variables = data.select_dtypes(include = ['category', 'object']).columns.tolist()\n",
    "    dummy_variables = pd.get_dummies(data.loc[:,categorical_variables])\n",
    "\n",
    "    data = pd.merge(data, dummy_variables, left_index = True, right_index = True)\n",
    "    data_out = data.drop(categorical_variables, axis = 1)\n",
    "    \n",
    "    if to_print == 1:\n",
    "    \n",
    "        print (\"List of all categorical variables: \", \"\\n\")\n",
    "        print (*categorical_variables, sep = \"\\n\")\n",
    "        print (\" \")\n",
    "\n",
    "        print (\"After creating dummy variables, the list looks like this: \", \"\\n\")\n",
    "        print ( *data_out.columns.tolist(), sep = \"\\n\")\n",
    "        print (\" \")\n",
    "        \n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impact_coding(data, variable, new_variable_name):\n",
    "    \n",
    "#     import math\n",
    "#     from math import log\n",
    "    \n",
    "#     # WARNING: PLEASE DO NOT HAVE ANY NAs!!\n",
    "#     data[variable].fillna('NA', inplace = True)\n",
    "\n",
    "#     #variable = 'Embarked'\n",
    "#     #new_variable_name = 'Embarked_impact'\n",
    "\n",
    "#     response_variable = 'Survived'\n",
    "#     distinct_values = data[variable].unique().tolist()\n",
    "\n",
    "#     log_cols = [variable, new_variable_name]\n",
    "#     log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "#     for value in distinct_values:\n",
    "\n",
    "#         subset = data.loc[data[variable] == value,:]\n",
    "        \n",
    "#         #probability of 1 for this level\n",
    "#         level_summary = subset[[variable, response_variable]].groupby([response_variable], as_index=False).count()\n",
    "        \n",
    "#         level_ones = level_summary.loc[level_summary[response_variable] == 1, variable]\n",
    "#         level_zeros = level_summary.loc[level_summary[response_variable] == 0, variable]\n",
    "\n",
    "#         if (level_ones.empty is False) and (level_zeros.empty is False):\n",
    "#             level_prob = level_ones.item()/(level_ones.item() + level_zeros.item())\n",
    "\n",
    "#         elif (level_ones.empty is False):\n",
    "#             level_prob = 1\n",
    "\n",
    "#         elif (level_zeros.empty is False):\n",
    "#             level_prob = 0\n",
    "        \n",
    "#         #probability of 1 for everything\n",
    "#         overall_summary = data[[variable, response_variable]].groupby([response_variable], as_index=False).count()\n",
    "        \n",
    "#         overall_ones = overall_summary.loc[overall_summary[response_variable] == 1, variable]\n",
    "#         overall_zeros = overall_summary.loc[overall_summary[response_variable] == 0, variable]\n",
    "        \n",
    "#         if (overall_ones.empty is False) and (overall_zeros.empty is False):\n",
    "#             overall_prob = overall_ones.item()/(overall_ones.item() + overall_zeros.item())\n",
    "\n",
    "#         elif (overall_ones.empty is False):\n",
    "#             overall_prob = 1\n",
    "\n",
    "#         elif (overall_zeros.empty is False):\n",
    "#             overall_prob = 0\n",
    "            \n",
    "#         if level_prob!=0\n",
    "#         impact = math.log(level_prob/(1-level_prob)) - math.log (overall_prob/(1-overall_prob))\n",
    "\n",
    "#         log_entry = pd.DataFrame([[value, impact]], columns=log_cols)\n",
    "#         log = log.append(log_entry)\n",
    "\n",
    "#     data_out = pd.merge(data, log[[variable,new_variable_name]],  on=variable)\n",
    "    \n",
    "#     return data_out, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impact_coding(data, variable, new_variable_name):\n",
    "    \n",
    "    #source: https://github.com/WinVector/vtreat/blob/master/R/effectTreatmentC.R\n",
    "    \n",
    "    import numpy as np\n",
    "    import scipy.special as spec\n",
    "    \n",
    "    epsilon = 1.0e-6\n",
    "    smFactor = 1.0e-4\n",
    "\n",
    "    # WARNING: PLEASE DO NOT HAVE ANY NAs!!\n",
    "    data[variable].fillna('NA', inplace = True)\n",
    "\n",
    "    #variable = 'Embarked'\n",
    "    #new_variable_name = 'Embarked_impact'\n",
    "\n",
    "    response_variable = 'Survived'\n",
    "    distinct_values = data[variable].unique().tolist()\n",
    "\n",
    "    log_cols = [variable, new_variable_name]\n",
    "    log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "    for value in distinct_values:\n",
    "\n",
    "        subset = data.loc[data[variable] == value,:]\n",
    "        \n",
    "        level_summary = subset[[variable, response_variable]].groupby([response_variable], as_index=False).count()\n",
    "        overall_summary = data[[variable, response_variable]].groupby([response_variable], as_index=False).count()\n",
    "        \n",
    "        true_cases = overall_summary.loc[overall_summary[response_variable] == 1, variable]\n",
    "        false_cases = overall_summary.loc[overall_summary[response_variable] == 0, variable]\n",
    "\n",
    "        if (true_cases.empty is False):\n",
    "            nTrue = true_cases.item()\n",
    "        else:\n",
    "            nTrue = 0\n",
    "            \n",
    "        if (false_cases.empty is False):\n",
    "            nFalse = false_cases.item()\n",
    "        else:\n",
    "            nFalse = 0\n",
    "            \n",
    "        true_cases_given_level = level_summary.loc[level_summary[response_variable] == 1, variable]\n",
    "        false_cases_given_level = level_summary.loc[level_summary[response_variable] == 0, variable]\n",
    "        \n",
    "        if (true_cases_given_level.empty is False):\n",
    "            nTrue_givenLevel = true_cases_given_level.item()\n",
    "        else:\n",
    "            nTrue_givenLevel = 0\n",
    "            \n",
    "        if (false_cases_given_level.empty is False):\n",
    "            nFalse_givenLevel = false_cases_given_level.item()\n",
    "        else:\n",
    "            nFalse_givenLevel = 0\n",
    "        \n",
    "        #unconditional probability that target is true\n",
    "        prior_probTrue = np.maximum(epsilon, np.minimum(1-epsilon, nTrue/(nTrue+nFalse)))\n",
    "        \n",
    "        #probability of a given evidence, condition on outcome = True\n",
    "        prob_givenTrue = (nTrue_givenLevel + smFactor)/(nTrue + smFactor)\n",
    "        \n",
    "        #probability of a given evidence, condition on outcome = False\n",
    "        prob_givenFalse = (nFalse_givenLevel + smFactor)/(nFalse + smFactor)\n",
    "        \n",
    "        #Bayes law, not normalized\n",
    "        \n",
    "        probTrue_givenLevel_unnorm = prior_probTrue * prob_givenTrue \n",
    "        probFalse_givenLevel_unnorm = (1-prior_probTrue) * prob_givenFalse \n",
    "        \n",
    "        probTrue_givenLevel = probTrue_givenLevel_unnorm / (probTrue_givenLevel_unnorm + probFalse_givenLevel_unnorm)\n",
    "        \n",
    "        impact = spec.logit (probTrue_givenLevel) - spec.logit(prior_probTrue)\n",
    "\n",
    "        log_entry = pd.DataFrame([[value, impact]], columns=log_cols)\n",
    "        log = log.append(log_entry)\n",
    "\n",
    "    data_out = pd.merge(data, log[[variable,new_variable_name]],  on=variable)\n",
    "    \n",
    "    return data_out, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
